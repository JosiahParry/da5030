---
title: "Practicum 1"
author: "Josiah Parry"
date: "2020-01-02"
output: 
  pdf_document: 
    latex_engine: xelatex
---

# Problem 1

> (0 pts) Download the data set Glass Identification Database along with its explanation. Note that the data file does not contain header names; you may wish to add those. The description of each column can be found in the data set explanation. This assignment must be completed within an R Markdown Notebook.


> (0 pts) Explore the data set as you see fit and that allows you to get a sense of the data and get comfortable with it.

```{r echo=TRUE, message=FALSE, warning=FALSE}
options(scipen = 17)
library(tidyverse)
library(rsample)
library(recipes)
colnames <- c("id", "ri", "na", "mg", "al", "si", "k", "ca", "ba", "fe", "type")

glass <- read_csv("data/glass.data", col_names = colnames)
 
skimr::skim_to_list(glass) %>%
  pluck(1) %>%
  knitr::kable()

```


> (5 pts) Create a histogram of column 2 (refractive index) and overlay a normal curve; visually determine whether the data is normally distributed. 

```{r}
# plotting hist over fr
ggplot(glass, aes(ri)) +
  geom_histogram(aes(y = ..density..), bins = 10) +
  stat_function(fun = dnorm,
                args = list(mean = mean(glass$na),
                            sd = sd(glass$na)))
```

```{r}
shapiro.test(glass$ri)
```


The shapiro test returns a p value < 0.001. We can tehn conclude that the distribution is in fact not normaly distributed.

> (5 pts) Does the k-NN algorithm require normally distributed data or is it a non-parametric method? Comment on your findings.

k-nearest neighbors is a non parametric model that does not require normally distributed data. There is only one hyper-parameter in this model and that is k. We determine k and no parameters are discovered. One should note that distances can be affected by the magnitude of the variables and as such input vectors should be rescaled in some manner.

> (5 pts) Identify any outliers for the columns using a z-score deviation approach, i.e., consider any values that are more than 2 standard deviations from the mean as outliers. Which are your outliers for each column? What would you do? Do not remove them the outliers. 

```{r}
outs <- glass %>% 
  mutate_at(vars(everything(), -id), scale) %>% 
  pivot_longer(cols = -id, names_to = "var", values_to = "z") %>% 
  filter(abs(z) > 2)

knitr::kable(count(outs, var, sort = TRUE))
```
Following a z-score approach, there are a total of `r nrow(outs)` outliers. Outliers should not be removed from the data. We have no valid explanation of _why_ they should be removed. They may very well be properly observed data and are in fact representative of the underlying distribution. Outliers should only be observed if a strong case for their removal can be made such as measurement error.

> (10 pts) After removing the ID column (column 1), normalize the numeric columns, except the last one, using z-score standardization. The last column is the glass type and so it is excluded.

> (10 pts) The data set is sorted, so creating a validation data set requires random selection of elements. Create a stratified sample where you randomly select 20% of each of the cases for each glass type to be part of the validation data set. The remaining cases will form the training data set.

knn is not a trained model. This question is misleading. There is no "validation" of a knn model. A knn model holds all data in memory and is the reason why it is not recommended at large scale. One must recalculate distances with each new observation.

In the training recipe we keep both the `type` and `id` columns for later use and validation.

```{r}
# stratified sample 
init_split <- initial_split(
  glass, 
  prop = .8, 
  strata = type
)

# extract the training set
train_df <- training(init_split)

# prepping recipe
train_rec <- recipe(type ~., data = train_df) %>% 
  # center and scale â€” i.e. "z-score normalization"
  step_center(all_numeric(), -type, -id) %>% 
  step_scale(all_numeric(), -type, -id) %>% 
  prep()
```


> (20 pts) Implement the k-NN algorithm in R (do not use an implementation of k-NN from a package) and use your algorithm with a k=6 to predict the glass type for the following two cases:

First, I create a function that calculates the euclidian distance for all observations.


```{r}
tidy_dist <- function(df, id_col) {
  
  mdist <- select(df, -{{ id_col }}) %>% 
    dist(diag = TRUE, upper = FALSE) %>% 
    as.matrix(labels = TRUE)

  
  ret <- as.data.frame(mdist) %>% 
    setNames(pull(df, id)) %>% 
    mutate(id = pull(df, id)) %>% 
    gather(item2, distance, -id) %>%
    as_tibble()
  
  # remove upper triangle
  ret[!upper.tri(mdist),] %>%
    # remove diag
    filter(distance != 0)
  
}
```

Next, we need to apply the normalization steps to the data.
```{r}
baked_train <- bake(train_rec, train_df)
```

Then I calculate distances for all observations using this new function.

```{r}
glass_dist <- tidy_dist(baked_train, id)
```

Then, to identify which type each observation actually belongs to I join it back to the original data set using the `id` column. I select only the `id` and `type` columns for simplicity and data reduction. Following, I identify the 6 observations with the smalled euclidian distance then assign a predicted class based on majority vote.

```{r}
# define mode function 
mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

```


```{r}
preds <- left_join(glass_dist, 
          select(glass, id, type)) %>% 
  group_by(item2) %>% 
  top_n(-6, distance) %>% 
  summarise(class = mode(type)) %>% 
  mutate(item2 = as.integer(item2)) %>% 
  left_join(select(glass, id, type), 
            by = c("item2" = "id"))
```

I visualize the confusion matrix with some help from `janitor`.

```{r}
janitor::tabyl(preds, class, type)
```
I calculate the accuracy below.

```{r}
mean(preds$class == preds$type)
```

> Use the whole normalized data set for this; not just the training data set. Note that you need to normalize the values of the new cases the same way as you normalized the original data.

In order to make predictions with the below observations I create a function to do so.

```{r}
classify_type <- function(new_data) {

  # create new data frame with training data and new data
  bind_rows(
    # pre-process new data
    bake(train_rec, new_data),
    # pre-process glass
    bake(train_rec, glass)
  ) %>% 
    # calc dist
    tidy_dist(id) %>% 
    # join glass back on for type
    left_join(select(glass, id, type), by = "id") %>% 
    # group for counts
    group_by(item2) %>%
    # identify 6 neighbors
    top_n(-6, distance)  %>% 
    # identify the pred class
    summarise(class = mode(type)) %>% 
    # identify only the new dat
    filter(item2 == 999) %>% 
    # pull out the underlying vector
    pull(class)
  
}
```


> RI = 1.51621 | 12.53 | 3.48 | 1.39 | 73.39 | 0.60 | 8.55 | 0.00 | Fe = 0.08
> RI = 1.5893 | 12.71 | 1.85 | 1.82 | 72.62 | 0.52 | 10.51 | 0.00 | Fe = 0.05

I create a new data frame with the above data. I assign the `id` value to `999` so I can identify these later.


```{r}
new_dat <- tribble(
  ~"ri", ~"na", ~"mg", ~"al", ~"si", ~"k", ~"ca", ~"ba", ~"fe",
  1.51621 , 12.53 , 3.48 , 1.39 , 73.39 , 0.60 , 8.55 , 0.00 , 0.08,
  1.5893 , 12.71 , 1.85 , 1.82 , 72.62 , 0.52 , 10.51 , 0.00 , 0.05
) %>% 
  mutate(id = 999)
```

I create the predictions using purrr. We must iterate because if we add other observations it creates an inacurate distance measure for the single observation.

```{r}
map_dbl(1:2, ~{
  classify_type(
    slice(new_dat, .x)
    )
  } 
)
```


> (10 pts) Apply the knn function from the class package with k=6 and redo the cases from Question (7). Compare your answers.

Below I use the `class::knn()` classifier to generate predictions. 

```{r}
library(class)

baked_test <- testing(init_split) %>% 
  bake(train_rec, .)

class_preds <- knn(
  train = select(baked_train, -id, -type), 
  test = select(baked_test, -type, -id),
  cl = pull(baked_train, type),
  k = 6)
```

I define my own testing set and perform my own predictions below. Note that the preprocessing occurs within the `classify_type()` function.

```{r}
testing_set <- testing(init_split) %>% 
  mutate(id = 999) 

my_preds <- map_dbl(1:nrow(testing_set), ~{
  classify_type(
    slice(testing_set, .x))
  }
)
```

I calculate the accuracy of each classifier.

```{r}
mean(class_preds == baked_test$type)
mean(my_preds == baked_test$type)
```


> (10 pts) Using your own implementation as well as the class package implementation of kNN, create a plot of k (x-axis) from 2 to 10 versus error rate (percentage of incorrect classifications) for both algorithms using ggplot.

```{r}
# create function for making predictions and calc acc
glass_knn <- function(k) {
  preds <- left_join(glass_dist, 
                     select(glass, id, type)) %>% 
    group_by(item2) %>% 
    top_n(-k, distance) %>% 
    summarise(class = mode(type)) %>% 
    mutate(item2 = as.integer(item2)) %>% 
    left_join(select(glass, id, type), 
              by = c("item2" = "id"))
  # calculate acc and "error"
  # should really be specificity and sensitivity
  preds %>% 
    summarise(accuracy = mean(class == type),
              misclass = 1 - accuracy)
  
}


```

```{r}
k_preds <- map_dfr(2:10, ~{
  class_preds <- knn(
    train = select(baked_train, -id, -type), 
    test = select(baked_test, -type, -id),
    cl = pull(baked_train, type),
    k = .x)
  
  tibble(
    k = .x,
    accuracy = mean(class_preds == baked_test$type),
    misclass = 1 - accuracy,
    method = "class"
  )
  
}) %>% 
  bind_rows(
    map_dfr(2:10, glass_knn, .id = "k") %>% 
  mutate(k = as.integer(k) + 1,
         method = "mine")
  )


k_preds %>%
  ggplot(aes(k, misclass, color = method)) +
  geom_line() +
  geom_point()
```


> (10 pts) Produce a cross-table confusion matrix showing the accuracy of the classification using knn from the class package with k = 5.

```{r}
class_preds <- knn(
  train = select(baked_train, -id, -type), 
  test = select(baked_test, -type, -id),
  cl = pull(baked_train, type),
  k = 5)

table(class_preds, baked_test$type)
```

> (10 pts) Download this (modified) version of the Glass data set containing missing values in column 4. Identify the missing values. Impute the missing values using your version of kNN from Problem 2 below using the other columns are predictor features.

I am unsure specifically what is being asked. I am going to work under the assumption that it is desired to impute the mp values into the newly downloaded data using our self-created KNN algorithm where the "training" data is the full glass data set and the "test" set is the missing values. 

Since the type of imputation was not specified I will impute the average `mg` value.

```{r}
new_glass <- read_csv("data/glass_missing.csv",
                      col_names = colnames) %>% 
  mutate(id = id + 900)

# skim data to find missingness
skimr::skim(new_glass)

# create df with only missing data
mg_missing <- filter(new_glass, is.na(mg))
```


```{r}
# create general function to find the neighbors 
find_neighbors <- function(new_data, k) {
  new_data_ids <- new_data$id
  # create new data frame with training data and new data
  bind_rows(
    # pre-process new data
    bake(train_rec, new_data),
    # pre-process glass
    bake(train_rec, glass)
  ) %>% 
    # calc dist
    tidy_dist(id) %>% 
    # join glass back on for type
    left_join(select(glass, id, type), by = "id") %>% 
    filter(item2 %in% new_data_ids) %>% 
    # group for counts
    group_by(item2) %>%
    # identify 6 neighbors
    top_n(-k, distance)  %>% 
    arrange(desc(item2)) %>% 
    select(neighbor = id, input_id = item2) %>% 
    select(2, 1) 
}

# iterate over missing rows
mg_neighbors <- map_dfr(1:nrow(mg_missing), ~{
  find_neighbors(
    slice(mg_missing, .x),
    6)
}
)

# take 6 closest neighbors and calculate mean mg values
imputed_mg <- left_join(mg_neighbors, glass, by = c("neighbor" = "id")) %>% 
  group_by(input_id) %>% 
  summarise(mg = mean(mg))
  
# bind rows and all is well.
all_imputed <- mg_missing %>% 
  mutate(mg = pull(arrange(imputed_mg, as.integer(input_id)), mg)) %>% 
  bind_rows(
    filter(new_glass, !id %in% pull(mg_missing, id))
    )
```


# Problem 2

> 1. (0 pts) Investigate this data set of home prices in King County (USA).

> 2. (5 pts) Save the price column in a separate vector/dataframe called target_data. Move all of the columns except the ID, date, price, yr_renovated, zipcode, lat, long, sqft_living15, and sqft_lot15 columns into a new data frame called train_data.


```{r}
# loading data.table for performance
library(data.table)

housing <- read_csv("data/kc_house_data.csv") %>% 
  select(-c(id, date, yr_renovated, zipcode, lat, long, sqft_living15, sqft_lot15))
```


> 3. (5 pts) Normalize all of the columns (except the boolean columns waterfront and view) using min-max normalization.

As this is extremely computationally intensive to calculate the distances, I will do this on a sample.

```{r}
house_rec <- recipe(price ~ ., data = housing,
                    retain = TRUE) %>% 
  step_range(contains("sqft"), 
             min = 0, max = 1) %>% 
  prep()

# preprocessed
house_pp <- juice(house_rec) %>% 
  mutate(id = row_number())

house_split <- initial_split(house_pp, .5)
```


```{r}
# define distance function using data.table
dt_dist <- function(df, id_col) {
  mdist <- select(df, - {{ id_col }}) %>% 
    dist(diag = TRUE, upper = FALSE) %>% 
    as.matrix(labels = TRUE)
  
  ret <- as.data.table(mdist) %>% 
    setNames(as.character(pull(df, {{ id_col }}))) %>% 
    .[, id := 1:nrow(.)] %>% 
    setkey(id) %>% 
    melt(id.vars = "id",
         variable.name = "item2", 
         value.name = "distance") %>% 
    as_tibble()
  
  ret[!upper.tri(mdist),] %>%
    # remove diag
    filter(distance != 0)
}
```

```{r}
new_df <- tibble(
  bedrooms = 4,
  bathrooms = 3,
  sqft_living = 4852,
  sqft_lot = 10244,
  floors = 3,
  waterfront = 0,
  view = 1,
  condition = 3,
  grade = 11, 
  sqft_above = 1960,
  sqft_basement = 820,
  yr_built = 1978
)

full_df <- bake(house_rec, new_df) %>% 
  mutate(id = 999) %>% 
  bind_rows(training(house_split))

house_dist <- dt_dist(full_df, id)

k <- 4 

top_k <- house_dist %>% 
  group_by(item2) %>% 
  top_n(distance, k)
  
  
```



> 4. (15 pts) Build a function called knn.reg that implements a regression version of kNN that averages the prices of the k nearest neighbors using a weighted average where the weight is 3 for the closest neighbor, 2 for the second closest and 1 for the remaining neighbors (recall that a weighted average requires that you divide the sum product of the weight and values by the sum of the weights).

> It must use the following signature:

> knn.reg(new_data, target_data, train_data, k)

> where new_data is a data frame with new cases, target_data is a data frame with a single column of prices from (2), train_data is a data frame with the features from (2) that correspond to a price in target_data, and k is the number of nearest neighbors to consider. It must return the predicted price.

Note that using a period in a function name denotes the use of an S3 class and as such can create issues in class differentiation. For this reason I will not use a period in the function name. Please see the [Syntax section](https://style.tidyverse.org/syntax.html#object-names) of the R Style Guide. 


(5 pts) Forecast the price of this new home using your regression kNN using k = 4:
bedrooms = 4 | bathrooms = 3 | sqft_living = 4852 | sqft_lot = 10244 | floors = 3 | waterfront = 0 | view = 1 | condition = 3 | grade = 11
sqft_above = 1960 | sqft_basement = 820 | yr_built = 1978

```{r}
sessionInfo()
```

